{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from GPy.inference.latent_function_inference import EPDTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = np.load('binary_class_ds.npz')\n",
    "X_crabs_train = raw_data['X_crabs_train']\n",
    "Y_crabs_train = raw_data['Y_crabs_train']\n",
    "X_liver_train = raw_data['X_liver_train']\n",
    "Y_liver_train = raw_data['Y_liver_train']\n",
    "X_diabetes_train = raw_data['X_diabetes_train']\n",
    "Y_diabetes_train = raw_data['Y_diabetes_train']\n",
    "X_banana_train = raw_data['X_banana_train']\n",
    "Y_banana_train = raw_data['Y_banana_train']\n",
    "X_iono_train = raw_data['X_iono_train']\n",
    "Y_iono_train = raw_data['Y_iono_train']\n",
    "X_sonar_train = raw_data['X_sonar_train']\n",
    "Y_sonar_train = raw_data['Y_sonar_train']\n",
    "X_cancer_train = raw_data['X_cancer_train']\n",
    "Y_cancer_train = raw_data['Y_cancer_train']\n",
    "X_heart_train = raw_data['X_heart_train']\n",
    "Y_heart_train = raw_data['Y_heart_train']\n",
    "X_ringnorm_train = raw_data['X_ringnorm_train']\n",
    "Y_ringnorm_train = raw_data['Y_ringnorm_train']\n",
    "X_twonorm_train = raw_data['X_twonorm_train']\n",
    "Y_twonorm_train = raw_data['Y_twonorm_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "print Y_cancer_train.shape\n",
    "Y_cancer_train= Y_cancer_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train = X_banana_train\n",
    "# Y_train = Y_banana_train\n",
    "X_train = X_cancer_train\n",
    "Y_train = Y_cancer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# kern_f = GPy.kern.RBF(X_train.shape[1], variance=4, lengthscale=(X_train.max(axis=0) - X_train.min(axis=0))/100.0, ARD=True, name='f_rbf') \n",
    "kern_f = GPy.kern.RBF(X_train.shape[1], variance=10, lengthscale=1.0, ARD=True, name='f_rbf') \n",
    "kern_f += GPy.kern.Bias(X_train.shape[1], variance=0.1, name='f_bias')\n",
    "kern_f += GPy.kern.White(X_train.shape[1], variance=1e-5, name='f_white')\n",
    "\n",
    "lik = GPy.likelihoods.Bernoulli()\n",
    "kern_f.name = 'f_kern'\n",
    "\n",
    "num_inducing = int(X_train.shape[0]*0.10)\n",
    "Z = np.random.permutation(X_train.copy())[:num_inducing, :]\n",
    "m_svgp1 = GPy.core.SVGP(X=X_train.copy(), Y=Y_train.copy(), Z=Z.copy(), kernel=kern_f.copy(), likelihood=lik.copy(), mean_function=None, Y_metadata=None, name='svgp_crabs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : svgp_crabs\n",
      "Objective : 1755.91085772\n",
      "Number of Parameters : 3365\n",
      "Number of Optimization Parameters : 3365\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1msvgp_crabs.             \u001b[0;0m  |      value  |  constraints  |  priors\n",
      "  \u001b[1minducing_inputs         \u001b[0;0m  |   (56, 30)  |               |        \n",
      "  \u001b[1mf_kern.f_rbf.variance   \u001b[0;0m  |       10.0  |      +ve      |        \n",
      "  \u001b[1mf_kern.f_rbf.lengthscale\u001b[0;0m  |      (30,)  |      +ve      |        \n",
      "  \u001b[1mf_kern.f_bias.variance  \u001b[0;0m  |        0.1  |      +ve      |        \n",
      "  \u001b[1mf_kern.f_white.variance \u001b[0;0m  |      1e-05  |      +ve      |        \n",
      "  \u001b[1mq_u_chol                \u001b[0;0m  |  (1596, 1)  |               |        \n",
      "  \u001b[1mq_u_mean                \u001b[0;0m  |    (56, 1)  |               |        \n"
     ]
    }
   ],
   "source": [
    "print(m_svgp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paramz.optimization.optimization.opt_lbfgsb at 0x7f012a7206d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_svgp1.optimize('lbfgs', max_iters=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name : svgp_crabs\n",
      "Objective : 462.475091024\n",
      "Number of Parameters : 3365\n",
      "Number of Optimization Parameters : 3365\n",
      "Updates : True\n",
      "Parameters:\n",
      "  \u001b[1msvgp_crabs.             \u001b[0;0m  |             value  |  constraints  |  priors\n",
      "  \u001b[1minducing_inputs         \u001b[0;0m  |          (56, 30)  |               |        \n",
      "  \u001b[1mf_kern.f_rbf.variance   \u001b[0;0m  |    0.298801078556  |      +ve      |        \n",
      "  \u001b[1mf_kern.f_rbf.lengthscale\u001b[0;0m  |             (30,)  |      +ve      |        \n",
      "  \u001b[1mf_kern.f_bias.variance  \u001b[0;0m  |   0.0830643299478  |      +ve      |        \n",
      "  \u001b[1mf_kern.f_white.variance \u001b[0;0m  |  9.9984549431e-06  |      +ve      |        \n",
      "  \u001b[1mq_u_chol                \u001b[0;0m  |         (1596, 1)  |               |        \n",
      "  \u001b[1mq_u_mean                \u001b[0;0m  |           (56, 1)  |               |        \n"
     ]
    }
   ],
   "source": [
    "print(m_svgp1)\n",
    "# print(m_svgp1['f_kern.f_rbf.lengthscale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_svgp_model(X_train, Y_train, Z=None, percent=0.15):\n",
    "    kern_f = GPy.kern.RBF(X_train.shape[1], variance=10.0, lengthscale=1.0, ARD=True, name='f_rbf') \n",
    "#     kern_f = GPy.kern.RBF(X_train.shape[1], variance=10.0, lengthscale=(X_train.max(axis=0) - X_train.min(axis=0))/6.0, ARD=True, name='f_rbf') \n",
    "    kern_f += GPy.kern.Bias(X_train.shape[1], variance=0.1, name='f_bias')\n",
    "    kern_f += GPy.kern.White(X_train.shape[1], variance=1e-5, name='f_white')\n",
    "\n",
    "    lik = GPy.likelihoods.Bernoulli()\n",
    "    kern_f.name = 'f_kern'\n",
    "    num_inducing = int(X_train.shape[0]*percent)\n",
    "    print \"percentage of inducing points:\", percent\n",
    "    Z = np.random.permutation(X_train.copy())[:num_inducing, :]\n",
    "    m_svgp = GPy.core.SVGP(X=X_train.copy(), Y=Y_train.copy(), Z=Z.copy(), kernel=kern_f.copy(), likelihood=lik.copy(), mean_function=None, Y_metadata=None, name='svgp_crabs')\n",
    "    return m_svgp  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_epdtc_model(X_train, Y_train, Z=None, percent=0.15):\n",
    "    kern_f = GPy.kern.RBF(X_train.shape[1], variance=10.0, lengthscale=1.0, ARD=True, name='f_rbf') \n",
    "    kern_f += GPy.kern.Bias(X_train.shape[1], variance=0.1, name='f_bias')\n",
    "    kern_f += GPy.kern.White(X_train.shape[1], variance=1e-5, name='f_white')\n",
    "    \n",
    "    epdtc = GPy.inference.latent_function_inference.EPDTC(ep_mode='alternated',  parallel_updates=True)\n",
    "    lik = GPy.likelihoods.Bernoulli()\n",
    "    kern_f.name = 'f_kern'\n",
    "    num_inducing = int(X_train.shape[0]*percent)\n",
    "    print \"percentage of inducing points:\", percent\n",
    "    Z = np.random.permutation(X_train.copy())[:num_inducing, :]\n",
    "#     m_epdtc = GPy.core.SparseGP(X=X_train.copy(), Y=Y_train.copy(), Z=Z.copy(), kernel=kern_f.copy(), likelihood=lik.copy(), inference_method=epdtc, mean_function=None, Y_metadata=None, name='epdtc')\n",
    "    m_epdtc = GPy.core.SparseGPClassification(X=X_train.copy(), Y=Y_train.copy(), Z=Z.copy(), kernel=kern_f.copy(), likelihood=lik.copy(), inference_method=epdtc, mean_function=None, Y_metadata=None, name='epdtc')\n",
    "    return m_epdtc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GPy.util.classification\n",
    "\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, X, Y, Xtest=None, Ytest=None, K=10, seed=42):\n",
    "        X_orig = X.copy()\n",
    "        Y_orig = Y.copy()\n",
    "        np.random.seed(seed=seed)\n",
    "        if Y.ndim == 1:\n",
    "            Y = Y.reshape(-1,1)\n",
    "        \n",
    "        randomize = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randomize)\n",
    "        X = X[randomize,:]\n",
    "        Y = Y[randomize,:]\n",
    "        self.K = K\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.m_list= []\n",
    "        self.X_test_list = []\n",
    "        self.Y_test_list = []\n",
    "        self.partition_flag = False\n",
    "        self.model_fn_flag = False\n",
    "    \n",
    "    def split_data(self):\n",
    "        X_train_list = []\n",
    "        Y_train_list = []\n",
    "        num_batch = self.X.shape[0]/self.K\n",
    "        rem = self.X.shape[0] % self.K\n",
    "        \n",
    "\n",
    "        for i in range(self.K-1):\n",
    "            X_batch = self.X[i*num_batch:(i+1)*num_batch,:]  \n",
    "            Y_batch = self.Y[i*num_batch:(i+1)*num_batch,:]\n",
    "            X_train_list.append(X_batch)\n",
    "            Y_train_list.append(Y_batch)\n",
    "        X_train_list.append(self.X[(self.K-1)*num_batch:self.K*num_batch+rem,:])\n",
    "        Y_train_list.append(self.Y[(self.K-1)*num_batch:self.K*num_batch+rem,:])\n",
    "        self.X_train_list = X_train_list\n",
    "        self.Y_train_list = Y_train_list\n",
    "        self.partition_flag = True\n",
    "        \n",
    "    def show_data_partition(self):\n",
    "        if self.partition_flag is False:\n",
    "            self.split_data()\n",
    "        for i in self.X_train_list:\n",
    "            print len(i)\n",
    "    \n",
    "    def set_model_function(self, fn, **kwargs):\n",
    "        self.get_model_fn = fn\n",
    "        self.fn_args = dict()\n",
    "        self.fn_kwargs = kwargs\n",
    "        self.model_fn_flag = True\n",
    "    \n",
    "    def get_model_function(self):\n",
    "        return self.get_model_fn\n",
    "\n",
    "    def get_model_list(self):\n",
    "        return self.m_list\n",
    "    \n",
    "    def get_test_set(self, i):\n",
    "        return self.X_test_list[i], self.Y_test_list[i]\n",
    "    \n",
    "    def train_KFold(self):\n",
    "        if self.partition_flag is False:\n",
    "            self.split_data()\n",
    "        \n",
    "        if self.model_fn_flag is False:\n",
    "            self.set_model_function()\n",
    "        \n",
    "        for i in range(self.K):\n",
    "            Xtest = self.X_train_list.pop(i)\n",
    "            Ytest =  self.Y_train_list.pop(i)\n",
    "            Xtrain_all = np.concatenate(self.X_train_list, axis=0)\n",
    "            Ytrain_all = np.concatenate(self.Y_train_list, axis=0)\n",
    "            self.X_train_list.insert(i, Xtest)\n",
    "            self.Y_train_list.insert(i, Ytest)\n",
    "            self.X_test_list.append(Xtest)\n",
    "            self.Y_test_list.append(Ytest)\n",
    "#             m = get_svgp_model(Xtrain_all, Ytrain_all)\n",
    "            m = self.get_model_fn(Xtrain_all, Ytrain_all, **self.fn_kwargs)\n",
    "            m.optimize('lbfgs', max_iters=900)\n",
    "            self.m_list.append(m)\n",
    "\n",
    "\n",
    "\n",
    "class EvalClassification():\n",
    "    def __init__(self, m):\n",
    "        self.m = m\n",
    "    \n",
    "    def NLL(self):\n",
    "        return self.m.log_likelihood()\n",
    "    \n",
    "    def TestNLL1(self, Xtest, Ytest):\n",
    "        mu = self.m._raw_predict(Xtest)\n",
    "        predict_mean = self.m.likelihood.gp_link.transf(mu)\n",
    "        target = Ytest\n",
    "        probs, probs_var = self.m.predict(Xtest)\n",
    "        nlprobs = np.log(probs)\n",
    "        nlprobsvar = np.log(probs_var)\n",
    "        NLP = np.mean(np.log(probs), axis=0)\n",
    "#         NLP_Var = np.median(np.log(nlprobsvar), axis=0)\n",
    "        self.probs = probs\n",
    "        return probs, NLP\n",
    "    \n",
    "    def TestNLL(self, Xtest, Ytest, median=False):\n",
    "        mu, var = self.m._raw_predict(Xtest)\n",
    "        nll = -stats.norm.logcdf(1.0 * Ytest * mu / np.sqrt(1 + var))\n",
    "        if median:\n",
    "            val= np.median(nll)\n",
    "        else:\n",
    "            val= np.mean(nll)\n",
    "        sigma2 = 2*np.nanstd(nll)\n",
    "        return val, sigma2\n",
    "        \n",
    "    \n",
    "    def conf_matrix(self, Xtest, Ytest, threshold=0.5):\n",
    "        assert self.probs.size == Ytest.size\n",
    "        decision = np.zeros((Ytest.size, 1))\n",
    "        decision[self.probs>threshold] = 1\n",
    "        diff = decision - Ytest\n",
    "        f_0 = diff[diff==-1].size\n",
    "        f_1 = diff[diff==1].size    \n",
    "        t_1 = np.sum(decision[diff ==0])\n",
    "        t_0 = Ytest.size - f_0 - f_1 - t_1\n",
    "        correct = t_0+t_1\n",
    "        false = Ytest.size -1\n",
    "        error = (f_0 + f_1)/ np.float(Ytest.size)\n",
    "        return error, f_0, f_1, t_0, t_1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is just very easy to use. The above classes define the training and validation procedure with K-Fold Cross validation. I can just use any function which returns me a sparse GP model(either SVGP or EPDTC), and then I can just train and evaluate with help of the classes defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "113\n",
      "113\n",
      "113\n",
      "117\n",
      "percentage of inducing points: 0.16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'SparseGPClassification'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-78ee6f9f32af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mds1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_data_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mds1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_epdtc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mds1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_KFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-54aebd4b7c29>\u001b[0m in \u001b[0;36mtrain_KFold\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_test_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#             m = get_svgp_model(Xtrain_all, Ytrain_all)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e5d44fabcd15>\u001b[0m in \u001b[0;36mget_epdtc_model\u001b[0;34m(X_train, Y_train, Z, percent)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_inducing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     m_epdtc = GPy.core.SparseGP(X=X_train.copy(), Y=Y_train.copy(), Z=Z.copy(), kernel=kern_f.copy(), likelihood=lik.copy(), inference_method=epdtc, mean_function=None, Y_metadata=None, name='epdtc')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mm_epdtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseGPClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkern_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlik\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepdtc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epdtc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm_epdtc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'SparseGPClassification'"
     ]
    }
   ],
   "source": [
    "ds1 = Dataset(X=X_train, Y=Y_train, K=5)\n",
    "ds1.split_data()\n",
    "ds1.show_data_partition()\n",
    "ds1.set_model_function(get_epdtc_model, percent=0.16)\n",
    "ds1.train_KFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "56\n",
      "65\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n",
      "percentage of inducing points: 0.12\n"
     ]
    }
   ],
   "source": [
    "ds1 = Dataset(X=X_train, Y=Y_train, K=10)\n",
    "ds1.split_data()\n",
    "ds1.show_data_partition()\n",
    "ds1.set_model_function(get_svgp_model, percent=0.12)\n",
    "ds1.train_KFold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.553660201543\n",
      "0.206344864837\n",
      "0.372637362637\n",
      "[0.54858110072053112, 0.56271607478895114, 0.55294604154202942, 0.54938564838796633, 0.55102422681373353, 0.55648216565882869, 0.54919883862715024, 0.56147910054350336, 0.63325760332040382, 0.55437436154349351]\n"
     ]
    }
   ],
   "source": [
    "test_nll_list = []\n",
    "test_nll_list2 = []\n",
    "error_list= []\n",
    "sigma2_list = []\n",
    "\n",
    "for ind, mi in enumerate(ds1.get_model_list()):\n",
    "    ec = EvalClassification(mi)\n",
    "    xtest, ytest = ds1.get_test_set(ind)\n",
    "    testnll1, sigma2 = ec.TestNLL(xtest, ytest)\n",
    "    \n",
    "    probs2, testnll2  = ec.TestNLL1(xtest, ytest)\n",
    "#     testnll2 = ec.TestNLL1(xtest, ytest)\n",
    "    error, f_0, f_1, t_0, t_1 = ec.conf_matrix(xtest, ytest)\n",
    "    test_nll_list.append(testnll1)\n",
    "    sigma2_list.append(sigma2)\n",
    "    error_list.append(error)\n",
    "\n",
    "print np.median(test_nll_list)\n",
    "print np.median(sigma2_list)\n",
    "print np.mean(error_list)\n",
    "print test_nll_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gpy27-devel]",
   "language": "python",
   "name": "conda-env-gpy27-devel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
